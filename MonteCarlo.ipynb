{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações e funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, QLearningAgent, MonteCarloAgent\n",
    "from environment import Environment\n",
    "from screen import Screen\n",
    "\n",
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11\n",
    "\n",
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()\n",
    "\n",
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n",
    "\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monte_carlo(agent: Agent, reward_function, episodes, display, speed, verbose=True):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "                    \n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            decoded_action1 = decode_action(action1)\n",
    "            decoded_action2 = decode_action(action2)\n",
    "            agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "            \n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "        \n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=200, speed=0, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
